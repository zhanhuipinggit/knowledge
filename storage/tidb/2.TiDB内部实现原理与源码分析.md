# TiDB 概述与架构设计

## 2.1 TiDB 的 SQL 处理与调度
- ### SQL 解析与优化
 TiDB 的 SQL 解析和优化过程可以分为几个步骤：SQL 解析、查询优化和执行引擎的处理。
- #### SQL 解析
  - 在 TiDB 中，SQL 解析的主要任务是将 SQL 查询转换为结构化的抽象语法树（AST）。TiDB 使用了 Parser 组件来进行 SQL 解析。
    - Parser 组件：TiDB 使用 Go 语言的 parser 包，主要负责将 SQL 字符串解析成抽象语法树（AST）。AST 是 SQL 查询的内部表示形式，包含了 SQL 的所有结构信息。 
- TiDB 使用的 SQL 解析器是由 github.com/pingcap/parser 提供的。它是 TiDB 中 SQL 解析的核心模块。
```go
import "github.com/pingcap/parser"
// Parse SQL query to AST
stmt, err := parser.New().ParseOneStmt("SELECT * FROM t WHERE id = 1", "", "")
if err != nil {
log.Fatal(err)
}
fmt.Println(stmt) // 输出 AST
 
```
上述代码通过 ParseOneStmt 函数将 SQL 查询解析为一个抽象语法树（AST）。stmt 是一个代表 SQL 语句的结构体，可以进一步用于查询优化和执行。


- ### 查询优化器（Query Planner & Optimizer）
在 TiDB 中，查询优化是通过查询优化器（Optimizer）来完成的。查询优化的目标是根据查询的代价模型选择最佳的执行计划。

TiDB 的查询优化器使用了基于成本的优化策略（Cost-Based Optimization, CBO）。主要优化策略包括：

- **索引选择**：选择最适合的索引来加速查询。
- **表连接优化**：选择合适的连接顺序和连接方式（如 Nested Loop Join、Hash Join）。
- **查询重写**：对 SQL 语句进行等价重写

TiDB 的优化器的核心实现类是 planbuilder，它使用代价模型来评估不同查询计划的成本。

```go
import "github.com/pingcap/tidb/planner/core"

opt := core.NewOptimizer(ctx, tidb.QueryExecutor())
plan, err := opt.Optimize(stmt, tidb.GetAllTables())
if err != nil {
    log.Fatal(err)
}
fmt.Println(plan) // 输出优化后的查询计划

```
`core.NewOptimizer` 创建了一个新的优化器，Optimize 方法用于生成优化后的查询计划。查询计划会根据代价模型选择最优的执行计划。

- ### 执行引擎（Execution Engine）
执行引擎的任务是将优化后的查询计划转化为实际的执行步骤。执行引擎会根据查询计划向 TiKV 或 TiFlash 发起请求，获取数据并进行处理。

TiDB 执行引擎主要通过一个名为 executor 的模块来实现。执行引擎根据查询计划构建一个执行树，并逐层执行查询。
```go
import "github.com/pingcap/tidb/executor"

// 创建执行器
exec := executor.NewExecutor(ctx, plan)
result, err := exec.Execute()
if err != nil {
    log.Fatal(err)
}
fmt.Println(result) // 输出执行结果

```
`executor.NewExecutor` 用于创建一个执行器，Execute 方法用于执行查询计划并返回结果。执行器会根据不同的操作符（如表扫描、索引扫描等）调用 TiKV 和 TiFlash 进行数据读取和计算。

---
- ### TiDB 的内部组件
- #### TiKV 和 TiFlash
TiDB 的执行引擎通过 TiKV 和 TiFlash 来进行数据的存储和处理。
  - TiKV：TiKV 是 TiDB 的分布式事务存储引擎，它采用 Raft 协议保证数据一致性。TiDB 将 SQL 查询转化为 KV 操作，TiKV 提供数据存储和检索功能。
  - TiFlash：TiFlash 是 TiDB 的列存引擎，用于加速分析型查询，特别是支持高效的列式存储和 OLAP 查询。

在执行计划中，TiDB 会根据数据访问模式选择是否使用 TiKV 或 TiFlash。例如，TiFlash 可以用于执行大范围的聚合操作或多维度的查询。
- ### TiDB 查询执行流程
假设我们有一个 SQL 查询：
```sql
SELECT id, name FROM users WHERE age > 30 ORDER BY id LIMIT 10;

```
- **1、SQL 解析**：
TiDB 首先会通过 parser 组件将 SQL 字符串解析为 AST。
查询优化：
TiDB 会通过优化器选择最佳的执行计划，例如使用索引来加速 age > 30 的过滤，使用合适的排序算法（如排序合并）进行排序操作。
- **2、执行引擎**：
TiDB 生成的执行计划可能会涉及到 TiKV 或 TiFlash，执行引擎根据查询的性质选择合适的数据源。

最终，执行引擎会将查询请求分发到 TiKV 或 TiFlash 进行数据读取，并将结果返回给客户端。

---

## 2.2 TiKV 的存储与事务管理
### TiKV 数据存储结构

TiKV 是一个分布式的 Key-Value 存储系统，底层使用 RocksDB 作为存储引擎。RocksDB 是一个高效的嵌入式数据库，提供高性能的持久化存储。TiKV 基于 RocksDB 进行数据存储和索引的管理。
- **Key-Value 存储**：TiKV 将数据以键值对的形式存储，每个数据条目由一个键（Key）和一个值（Value）组成。TiKV 对键值对进行分布式管理，可以支持水平扩展。
- **RocksDB 存储引擎**：TiKV 使用 RocksDB 作为底层的存储引擎，负责物理存储和事务日志管理。RocksDB 提供高效的写入、读取和压缩算法。
  - **1. RocksDB 的写入流程**
    - **Write Request**：客户端向 RocksDB 发出写请求，数据被写入 MemTable。
    - **MemTable**：RocksDB 将写入的键值对保存在内存中的 MemTable（一个有序的哈希表或跳表）中。MemTable 是一个高效的内存数据结构，所有新的写入操作都会首先进入 MemTable。
    - **Write Ahead Log（WAL）**：同时，所有的写入操作会被记录到 Write-Ahead Log（WAL）中，保证数据的一致性。当发生崩溃恢复时，WAL 可以帮助重放写入操作。
    - **MemTable Flush to Disk**：当 MemTable 达到一定大小后，RocksDB 会将 MemTable 刷新（Flush）到磁盘，创建一个新的 SSTable 文件。
    - **Compaction**：当多个 SSTable 文件在磁盘中生成后，RocksDB 会通过压缩过程将这些 SSTable 合并，以减少存储占用并提高读取效率。
  - 以下是一个简化的 RocksDB 写入流程代码片段，模拟了向 RocksDB 写入数据的过程：
```go
import (
    "github.com/facebook/rocksdb"
    "log"
)

func writeToRocksDB(db *rocksdb.DB, key, value []byte) error {
    // 创建写操作对象
    writeOpts := rocksdb.NewWriteOptions()
    defer writeOpts.Destroy()

    // 执行写操作
    err := db.Put(writeOpts, key, value)
    if err != nil {
        return err
    }
    return nil
}

func main() {
    // 打开 RocksDB 数据库
    options := rocksdb.NewOptions()
    defer options.Destroy()
    options.SetCreateIfMissing(true)

    db, err := rocksdb.OpenDb(options, "/tmp/testdb")
    if err != nil {
        log.Fatal(err)
    }
    defer db.Close()

    // 写入数据
    err = writeToRocksDB(db, []byte("key1"), []byte("value1"))
    if err != nil {
        log.Fatal(err)
    }

    log.Println("Data written successfully!")
}

```
在这个例子中，db.Put 方法会将数据写入 MemTable，同时将写操作记录到 WAL 中。


- **2. RocksDB 的读取流程**

RocksDB 的读取流程是通过查找 MemTable 和 SSTable 文件来完成的，具体步骤如下：

  - **查找 MemTable**：RocksDB 首先会在 MemTable 中查找目标键。如果 MemTable 中找到了该键，返回对应的值。
  - **查找 SSTable**：如果 MemTable 中没有找到，RocksDB 会查找 SSTable 文件。SSTable 是存储在磁盘上的排序文件，RocksDB 会通过布隆过滤器（Bloom Filter）快速判断目标键是否存在于 SSTable 中，避免不必要的磁盘 I/O 操作。
  - **返回数据**：如果目标键存在于 MemTable 或 SSTable 中，RocksDB 会返回对应的值。如果都找不到，返回 NotFound 错误。
以下是一个简化的 RocksDB 读取流程代码片段，模拟了从 RocksDB 读取数据的过程：
```go
import (
"github.com/facebook/rocksdb"
"log"
)

func readFromRocksDB(db *rocksdb.DB, key []byte) ([]byte, error) {
// 创建读取选项
readOpts := rocksdb.NewDefaultReadOptions()
defer readOpts.Destroy()

    // 执行读取操作
    value, err := db.Get(readOpts, key)
    if err != nil {
        return nil, err
    }
    if value == nil {
        return nil, rocksdb.ErrNotFound
    }
    return value, nil
}

func main() {
// 打开 RocksDB 数据库
options := rocksdb.NewOptions()
defer options.Destroy()
options.SetCreateIfMissing(true)

    db, err := rocksdb.OpenDb(options, "/tmp/testdb")
    if err != nil {
        log.Fatal(err)
    }
    defer db.Close()

    // 读取数据
    value, err := readFromRocksDB(db, []byte("key1"))
    if err != nil {
        log.Fatal(err)
    }

    log.Printf("Data read successfully: %s", value)
}

```

- ** RocksDB 的压缩算法**

RocksDB 使用了 Level-Compaction 和 Universal-Compaction 两种压缩策略来优化存储和读取效率。

- **Level-Compaction**：这种策略通过将数据按照不同的 "Level" 进行组织，每个 Level 的数据量是有上限的。当某个 Level 的数据超出上限时，会触发压缩操作，将数据向下压缩到下一 Level。这种策略适用于写多读少的场景。

- **Universal-Compaction**：这种策略通过合并多个 SSTable 文件，减少存储的碎片并提高查询性能。它适用于写入负载较高的场景。

RocksDB 中的压缩操作是由后台线程自动执行的，确保在数据不断增加时，不会造成过多的 I/O 操作。

RocksDB 支持用户指定压缩算法，例如 Snappy、Zlib、LZ4 等。以下是设置压缩选项的代码：

```go
import (
    "github.com/facebook/rocksdb"
    "log"
)

func configureCompression() *rocksdb.Options {
    options := rocksdb.NewOptions()
    // 使用 Snappy 压缩算法
    options.SetCompression(rocksdb.SnappyCompression)
    return options
}

func main() {
    options := configureCompression()
    defer options.Destroy()

    // 打开 RocksDB 数据库
    db, err := rocksdb.OpenDb(options, "/tmp/testdb")
    if err != nil {
        log.Fatal(err)
    }
    defer db.Close()

    log.Println("RocksDB configured with Snappy compression")
}

```
###  RocksDB 写入与读取时序图（流程图）
下面是一个简化的写入和读取流程时序图，展示了数据从写入到读取的过程。我们使用 Mermaid 语法来描述时序图。
- 写入流程时序图
```mermaid
sequenceDiagram
    participant Client
    participant RocksDB
    participant MemTable
    participant WAL

    Client->>RocksDB: Put(key, value)
    RocksDB->>MemTable: Insert(key, value)
    RocksDB->>WAL: Write log (key, value)
    MemTable->>MemTable: Check if size exceeds threshold
    MemTable->>RocksDB: Flush MemTable to disk (SSTable)
    RocksDB->>Disk: Write SSTable

```
- 读取流程时序图
```mermaid
sequenceDiagram
    participant Client
    participant RocksDB
    participant MemTable
    participant SSTable
    participant BloomFilter

    Client->>RocksDB: Get(key)
    RocksDB->>MemTable: Lookup(key)
    alt Found in MemTable
        MemTable->>Client: Return value
    else Not found in MemTable
        RocksDB->>BloomFilter: Check Bloom Filter
        alt Found in Bloom Filter
            RocksDB->>SSTable: Lookup(key)
            SSTable->>Client: Return value
        else Not found
            RocksDB->>Client: Return NotFound error
        end
    end

```


### TiKV 和 Raft 协议

TiKV 是一个分布式键值数据库，支持水平扩展和强一致性。它使用了 **Raft 协议** 来保证数据的一致性和高可用性。TiKV 将数据分为多个 **Region**，每个 Region 大约包含 96MB 的数据。以下是关于 TiKV 如何通过 Raft 协议实现数据一致性和高可用性的详细解释：

#### 1. Region 设计

- **Region** 是 TiKV 数据的基本存储单位。每个 Region 大约 96MB，并且是 TiKV 中的一个逻辑分区。Region 存储的是一段连续的键值对。
- 每个 Region 会有多个副本（通常是 3 个副本），这些副本分布在集群中的不同节点上。副本之间的数据会通过 Raft 协议同步，确保数据的一致性。

-  Region 设计架构图（Mermaid）
```mermaid
graph TD
    A[TiKV Cluster] --> B[Region 1]
    A[TiKV Cluster] --> C[Region 2]
    A[TiKV Cluster] --> D[Region N]
    
    B --> E[Leader]
    B --> F[Follower]
    B --> G[Follower]
    
    C --> H[Leader]
    C --> I[Follower]
    C --> J[Follower]
    
    D --> K[Leader]
    D --> L[Follower]
    D --> M[Follower]

    %% 描述 Region 中的副本关系
    E -.-> F
    E -.-> G
    H -.-> I
    H -.-> J
    K -.-> L
    K -.-> M

```

- **解释**：

`TiKV Cluster` 包含多个 `Region`。
每个 `Region` 有一个 `Leader` 和多个 `Follower` 副本。
`Leader` 和 `Follower` 副本之间通过 Raft 协议同步数据（虚线表示副本之间的数据同步关系）。

- **时序图**

```mermaid
sequenceDiagram
    participant Client
    participant Leader as TiKV Leader
    participant Follower as TiKV Follower

    Client->>Leader: 发起写请求
    Leader->>Leader: 记录日志并同步
    Leader->>Follower: 同步日志
    Follower-->>Leader: 确认日志写入
    Leader->>Leader: 确保超过半数副本确认
    Leader->>Client: 返回成功响应

```
- **时序图说明：**

- 1、Client 发起一个写请求到 TiKV Leader。
- 2、Leader 将写请求记录到日志，并将日志同步到所有的 Follower 副本。
- 3、Follower 返回确认信息给 Leader。
- 4、当大多数副本（包括 Leader）确认日志成功后，Leader 向客户端返回成功响应。

**Region 管理（Region 请求的处理）**
```rust
struct Region {
    id: u64,
    leader: Option<u64>,  // 当前 Region 的 Leader 节点
    peers: Vec<Peer>,     // Region 的副本（包括 Leader 和 Follower）
}

impl Region {
    // 处理写请求
    fn handle_write(&mut self, key: String, value: String) -> Result<()> {
        // 创建写入日志条目
        let entry = Entry {
            key: key,
            value: value,
        };

        // 调用 Raft 协议进行日志提交
        if let Some(leader) = self.leader {
            leader.propose(entry)?;  // 提交给 Leader 处理
        }
        Ok(())
    }
}

```

- **解释：**

`Region` 结构体表示 TiKV 中的一个 Region，包含 Region 的 ID、Leader 节点以及副本（Peers）。
`handle_write() `方法处理写请求，并通过 Raft 协议将写请求提交到 Leader。



#### 2. Raft 协议概述

Raft 是一种一致性算法，设计用于分布式系统中保证日志的一致性和状态机的一致性。Raft 协议通过以下几个主要机制来保证数据的一致性和高可用性：

- **日志复制**：Raft 确保每个服务器上的日志序列是相同的，从而确保数据一致性。
- **选举机制**：Raft 保证集群中的一个节点始终处于领导者（Leader）角色，领导者负责处理所有的写操作和日志的复制。
- **副本同步**：Raft 会确保副本之间的数据一致性，所有写入请求必须先在 Leader 节点上进行，并通过日志复制到 Follower 节点。
- **故障恢复**：Raft 确保如果 Leader 发生故障，集群中的其他节点可以通过选举机制选举出新的 Leader，从而继续保证数据的一致性和可用性。

### 3. TiKV 如何利用 Raft 协议

TiKV 使用 Raft 协议来管理每个 Region 的副本，从而保证数据的一致性、可靠性和高可用性。具体来说，TiKV 通过以下方式实现：

#### a) 副本同步

- 每个 Region 会有多个副本（默认 3 个），这些副本分布在不同的 TiKV 节点上。
- Raft 协议确保所有副本的数据一致性。在 TiKV 中，副本分为 Leader 副本和 Follower 副本：
  - **Leader 副本**：负责接收客户端的写请求并将其记录到日志中，之后将日志同步到所有 Follower 副本。
  - **Follower 副本**：接收 Leader 副本同步过来的日志，并将数据持久化到磁盘。如果 Leader 副本故障，集群会选举一个新的 Leader 副本来继续服务。

#### b) 选举机制

- Raft 协议中的选举机制保证集群中始终有一个 Leader。当 TiKV 启动时，Region 会选择一个 Leader 节点来处理所有的写操作。
- 如果当前的 Leader 节点发生故障，集群中的其他副本会通过 Raft 的选举机制选举出一个新的 Leader，确保 TiKV 服务的持续可用性。

#### c) 日志复制

- TiKV 的写请求会首先发送到 Leader 节点，Leader 节点会将写入操作（写日志）复制到所有 Follower 副本。
- 当大多数副本（即超过半数）确认日志复制成功后，Leader 会向客户端返回操作成功的响应，保证数据的一致性。
- 如果某个副本由于网络延迟或故障未能及时接收到日志，Raft 协议会确保这些副本在恢复连接后会通过日志重放机制同步数据。

#### d) 数据恢复

- 如果 TiKV 中的某个节点发生故障，Raft 协议会确保失效的节点能够从其他节点的日志中恢复数据。
- 当一个新的 TiKV 节点加入集群或原来的节点重新加入时，Raft 会确保该节点通过复制操作获得当前最新的日志，并与集群中的其他节点保持一致。

### 4. TiKV 中的 Region 与 Raft 协议结合

- 每个 Region 维护着自己的 Raft 状态，包括 Leader 副本和 Follower 副本。
- 当 Region 的数据发生更新时，Raft 协议会确保 Leader 副本的数据更新被同步到 Follower 副本，以此来保持数据的一致性。
- TiKV 的 Region 可以跨多个 TiKV 实例（节点），这些节点通过 Raft 协议进行副本同步，确保每个 Region 在多个节点间的一致性和高可用性。

### 5. 故障恢复与高可用性

- TiKV 使用 Raft 协议的日志复制与选举机制来确保高可用性。如果一个 TiKV 节点故障或出现网络分区，其他节点可以通过选举机制快速选举出新的 Leader。
- TiKV 还提供了 **Region 移动** 和 **副本重建** 功能，确保在节点发生故障时，集群能够继续正常工作，同时进行数据恢复。



